import os
import cv2
import time
import json
import rclpy
import numpy as np
import math
import re
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
import tempfile
import threading

# å¼•å…¥ AgentFlow ä¾èµ–
from agentflow.agents.solver_embodied import construct_solver_embodied

# å¼•å…¥ ROS æ¶ˆæ¯
from simulator_messages.msg import SimulatorCommand  # è‡ªå®šä¹‰æ¶ˆæ¯
from .vln_connector import VLNConnector
from nav_msgs.msg import OccupancyGrid
from geometry_msgs.msg import Pose
from scipy.spatial.transform import Rotation as R

from .events import event_manager

class AgentBaseline(VLNConnector):
    def __init__(self):
        super().__init__()  # åˆå§‹åŒ– ROS Node + RGBD Subscriber

        # ä¸´æ—¶ç›®å½•
        self._temp_dir = tempfile.TemporaryDirectory()
        self.get_logger().info(f"Temporary directory created: {self._temp_dir.name}")
        self._lock = threading.Lock()  # æ¨ç†é”
        self._inference_thread = None

        load_dotenv(dotenv_path="agentflow/.env")
        self.get_logger().info(
            f"OpenAI Key Loaded: {'OPENAI_API_KEY' in os.environ}"
        )

        self.llm_engine_name = "gpt-4o"

        self.solver = construct_solver_embodied(
            llm_engine_name=self.llm_engine_name,
            enabled_tools=[
                "Base_Generator_Tool",
                "GroundedSAM2_Tool"
            ],
            tool_engine=["gpt-4o"],
            model_engine=["gpt-4o", "gpt-4o", "gpt-4o"],
            output_types="direct",
            max_time=300,
            max_steps=1,
            enable_multimodal=True
        )

        # prompts
        self.system_prompt = (
            "# Task Complete Criterior: Reach target within 3 meters\n"
            "# You will receive two images: \n"
            "## 1. First-person view (RGB).\n"
            "## 2. A decision value map:\n"
            "- Warm colors (red/yellow): preferred regions\n"
            "- Cold colors (blue): avoid if possiblen\n"
            "- Black: obstacles\n"
        )
        self.task_prompt = None
        self.data_prompt = None

        self.temp_dir = Path("tmp/agent_baseline")
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        self.step_counter = 0
        self.last_infer_step = -1  # é˜²æ­¢åŒä¸€å¸§é‡å¤æ¨ç†

        self.get_logger().info("AgentBaseline Initialized")

        self._stop_event = threading.Event()

        # map 
        self.raw_map_data = None
        self.raw_map_info = None
        self.raw_map_sub = self.create_subscription(
            OccupancyGrid,
            '/rtabmap/map',
            self.raw_map_callback,
            10
        )
        self.decision_map_data = None
        self.decision_map_sub = self.create_subscription(
            OccupancyGrid,
            '/agent/decision_costmap',
            self.decision_map_callback,
            10
        )
        self.latest_map_img = None

        # events
        event_manager.register("task_received", self.handle_task_received)

    # =====================================================
    # control logicï¼ˆone stepï¼‰
    # =====================================================
    def control_once_async(self):
        # å¦‚æœä¸Šä¸€è½®æ¨ç†è¿˜åœ¨æ‰§è¡Œï¼Œä¸å¯åŠ¨æ–°æ¨ç†
        if self._inference_thread is not None and self._inference_thread.is_alive():
            return

        # é€šè¿‡ get_observation è·å–æœ€æ–°è§‚æµ‹
        obs = self.get_observation()
        if obs is None:
            return  # æ•°æ®ä¸å®Œæ•´ï¼Œç›´æ¥è¿”å›

        rgb_snapshot = obs["rgb"].copy() if obs["rgb"] is not None else None
        depth_snapshot = obs["depth"].copy() if obs["depth"] is not None else None
        base_pose_snapshot = obs["base_pose"]  # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥ deepcopy

        # éé˜»å¡è°ƒç”¨ Inference
        self._inference_thread = threading.Thread(
            target=self.Inference,
            kwargs={
                "rgb": rgb_snapshot,
                "depth": depth_snapshot,
                "base_pose": base_pose_snapshot
            }
        )
        self._inference_thread.start()


    # =====================================================
    # Input Adapter
    # =====================================================
    def InputData(self, **kwargs):
        rgb_img = kwargs.get("rgb")
        
        # ä¿å­˜ç¬¬ä¸€è§†è§’ RGB
        rgb_path = self.temp_dir / f"rgb.jpg"
        cv2.imwrite(str(rgb_path), rgb_img)

        # ä¿å­˜ä¸Šå¸è§†è§’ Map (å¦‚æœæœ‰)
        img_paths = [str(rgb_path)]
        
        if self.latest_map_img is not None:
            map_path = self.temp_dir / f"map.jpg"
            cv2.imwrite(str(map_path), self.latest_map_img)
            img_paths.append(str(map_path)) # æ­¤æ—¶åˆ—è¡¨é‡Œæœ‰ä¸¤å¼ å›¾
            self.get_logger().info(f"Attached Map Image to LLM input")

        self.step_counter += 1
        return img_paths

    # =====================================================
    # Inference Adapter
    # =====================================================
    def Inference(self, **args):
        """
        é€šç”¨ LLM æ¨ç†æ¥å£ï¼ˆå¯æ¥æ”¶ä»»æ„è¾“å…¥ via **argsï¼‰
        çº¿ç¨‹å®‰å…¨ï¼Œè¿”å› SimulatorCommand
        """
        image_paths = args.get("image_paths", None)
        base_pose = args.get("base_pose", None)

        if image_paths is None:
            rgb = args.get("rgb")
            depth = args.get("depth")  # depth å¯ä»¥ç•™ç€ä»¥åç”¨
            if rgb is None:
                self.get_logger().warn("No RGB input for inference, skipping", throttle_duration_sec=2.0)
                return None
            image_paths = self.InputData(rgb=rgb, depth=depth)

        if self.task_prompt is not None:
            try:
                propmt = self.system_prompt + self.task_prompt
                output = self.solver.solve(
                    propmt,
                    image_paths=image_paths
                )

                raw_text = output.get("direct_output", "")
                cmd = self._parse_llm_to_ros(raw_text)

                if cmd is not None:
                    self.publish_simulator_command(cmd)
                    
                self.get_logger().info(f"[LLM] Thinking... input={image_paths[-1]}")
                return cmd
            
            except Exception as e:
                self.get_logger().error(f"Inference Error: {e}")
                return None
        else:
            self.get_logger().warning(f"Waiting for task", throttle_duration_sec=2.0)
        
    def destroy_node(self):
        super().destroy_node()
        self._temp_dir.cleanup()
        self.get_logger().info("Temporary directory cleaned up.")

    def handle_task_received(self, task:str):
        if task is None:
            self.get_logger().warning(f"Task is empty")
            return
        print(f"ğŸ¯ ä»»åŠ¡æ¥äº†")
        self.latest_task = task
        self.task_prompt = "[Updated Task]: " + task + "\n"

    def raw_map_callback(self, msg: OccupancyGrid):
        """ä¿å­˜åŸå§‹åœ°å›¾æ•°æ®"""
        h, w = msg.info.height, msg.info.width
        # åŸå§‹åœ°å›¾æ•°æ®ï¼š-1(æœªçŸ¥), 0(ç©ºé—²), 100(éšœç¢)
        self.raw_map_data = np.array(msg.data, dtype=np.int8).reshape(h, w)
        self.raw_map_info = msg.info
        
        # å°è¯•èåˆæ›´æ–°
        self._fuse_and_update_map_img()

    def decision_map_callback(self, msg: OccupancyGrid):
        """ä¿å­˜å†³ç­–åœ°å›¾æ•°æ®"""
        h, w = msg.info.height, msg.info.width
        # å†³ç­–åœ°å›¾æ•°æ®ï¼š0-100 (Cost)
        self.decision_map_data = np.array(msg.data, dtype=np.int8).reshape(h, w)
        
        # å°è¯•èåˆæ›´æ–°
        self._fuse_and_update_map_img()

    def _fuse_and_update_map_img(self):
        """æ ¸å¿ƒèåˆå‡½æ•°ï¼šå°†å†³ç­–çƒ­åŠ›å›¾ä¸åŸå§‹éšœç¢ç‰©é®ç½©å åŠ """
        if self.raw_map_data is None or self.decision_map_data is None:
            return

        # æ£€æŸ¥å°ºå¯¸æ˜¯å¦åŒ¹é… (é˜²æ­¢ RTAB-Map åŠ¨æ€æ‰©å›¾æ—¶å¯¼è‡´çš„ä¸ä¸€è‡´)
        if self.raw_map_data.shape != self.decision_map_data.shape:
            # å¦‚æœå°ºå¯¸ä¸ä¸€è‡´ï¼Œé€šå¸¸ä»¥ raw_map ä¸ºå‡†ï¼Œç­‰å¾… decision_map æ›´æ–°
            return

        # --- Step 1: åˆ¶ä½œåº•å›¾ (Decision Heatmap) ---
        # å½’ä¸€åŒ– cost (0-100) -> (0.0-1.0)
        # Cost è¶Šé«˜(100) -> è¶Šä¸æ¨è(Blue/Cold)
        # Cost è¶Šä½(0)   -> è¶Šæ¨è(Red/Warm)
        # OpenCV Jet: 0=Blue, 255=Red. æ‰€ä»¥æˆ‘ä»¬éœ€è¦åè½¬ Costã€‚
        
        # å°† int8 è½¬ float é˜²æ­¢æº¢å‡º
        cost_float = self.decision_map_data.astype(np.float32)
        
        # å½’ä¸€åŒ–å¹¶åè½¬: cost 0 -> val 1.0 (Red), cost 100 -> val 0.0 (Blue)
        heatmap_val = 1.0 - (cost_float / 100.0)
        heatmap_val = np.clip(heatmap_val, 0.0, 1.0)
        
        # è½¬ä¸º 0-255 å¹¶åº”ç”¨è‰²è°±
        heatmap_gray = (heatmap_val * 255).astype(np.uint8)
        fused_img = cv2.applyColorMap(heatmap_gray, cv2.COLORMAP_JET)

        # --- Step 2: åˆ¶ä½œé®ç½© (Raw Map Overlays) ---
        
        # æ©ç  A: éšœç¢ç‰© (Raw Map == 100) -> é»‘è‰²
        obstacle_mask = (self.raw_map_data == 100)
        fused_img[obstacle_mask] = [0, 0, 0]  # BGR = Black

        # æ©ç  B: æœªçŸ¥åŒºåŸŸ (Raw Map == -1) -> ç°è‰²
        unknown_mask = (self.raw_map_data == -1)
        fused_img[unknown_mask] = [128, 128, 128] # BGR = Grey

        # (å¯é€‰) æœºå™¨äººå½“å‰ä½ç½®æ ‡è®°ï¼Ÿ
        # é€šå¸¸ä¸éœ€è¦ï¼Œå› ä¸º LLM æ ¹æ®ç¬¬ä¸€äººç§°è§†è§’å’Œ odom åæ ‡èƒ½æ¨æ–­ï¼Œ
        # ä½†å¦‚æœåœ¨å›¾ä¸Šç”»ä¸ªå°ç®­å¤´æ•ˆæœä¼šæ›´å¥½ã€‚è¿™é‡Œå…ˆä¿æŒçº¯åœ°å›¾ã€‚

        self.latest_map_img = fused_img


    def _parse_llm_to_ros(self, output_text: str):
        cmd = SimulatorCommand()
        cmd.header.stamp = self.get_clock().now().to_msg()
        cmd.header.frame_id = "agent"
        
        method = ""
        method_params = ""

        # æå– **Action** æ ‡ç­¾åçš„æ–‡æœ¬
        action_match = re.search(
            r"(?:\*\*Action\*\*|Action|Navigation Goal)\s*:\s*(.*)",
            output_text,
            re.IGNORECASE | re.DOTALL
        )

        if action_match:
            action_text = action_match.group(1).strip()
            self.get_logger().info(f"Extracted Action Text: {action_text}")

            method_match = re.search(r"<(\w+)\((.*?)\)>", action_text, re.IGNORECASE | re.DOTALL)
            if method_match:
                method = method_match.group(1).strip()
                method_params = method_match.group(2).strip()
                self.get_logger().info(f"Parsed Method: {method}, Params: {method_params}")
            else:
                self.get_logger().warn("No <Method(...)> found in Action text.")
        else:
            self.get_logger().warn("Label 'Action:' not found in LLM output. No method extracted.")

        if method.lower() == "stop":
            self.get_logger().info("ğŸ Stop received, exiting baseline for restart")
            self._stop_event.set()

        cmd.method = method
        cmd.method_params = method_params

        return cmd

# =====================================================
# Main Loop
# =====================================================
def main(args=None):
    rclpy.init(args=args)
    node = AgentBaseline()

    try:
        while rclpy.ok():
            rclpy.spin_once(node, timeout_sec=0.05)  # æŒç»­åˆ·æ–°è®¢é˜…æ•°æ®
            node.control_once_async()               # éé˜»å¡æ¨ç†

            if node._stop_event.is_set():
                node.get_logger().info("ğŸ” Baseline reseting...")
                break
    except KeyboardInterrupt:
        pass
    finally:
        if node._inference_thread is not None:
            node.get_logger().info("Waiting for inference thread to finish...")
            node._inference_thread.join(timeout=2.0)

        node.destroy_node()
        rclpy.shutdown()


if __name__ == "__main__":
    main()
